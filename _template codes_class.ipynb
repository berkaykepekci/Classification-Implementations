{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roadmap:\n",
    "\n",
    "1.import\n",
    "2.label encoding\n",
    "3.get dummies\n",
    "4.missing value analyze (dropna or ycimpute)\n",
    "5.neighbor outliers (LocalOutlierFactor)\n",
    "6.singular outliers (quantile)\n",
    "7.automation\n",
    "8.model selection\n",
    "9.tune with GridSearchCV\n",
    "10.early stopping and visualization\n",
    "11.retune with early stopping\n",
    "12.predict\n",
    "13.getting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing value analyze with ycimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ycimpute.imputer import knnimput\n",
    "feat_names=list(df)\n",
    "ndf=np.array(df)\n",
    "s1=knnimput.KNN(k = 5).complete(ndf)\n",
    "s1=s1.round()\n",
    "df=pd.DataFrame(s1,columns=feat_names)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof=LocalOutlierFactor()\n",
    "lof.fit_predict(df)\n",
    "scores=lof.negative_outlier_factor_\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### singular outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=df.price.quantile(0.25)\n",
    "q3=df.price.quantile(0.75)\n",
    "iqr=q3-q1\n",
    "down_limit=(q1-iqr*1.5)\n",
    "up_limit=(q3+iqr*1.5)\n",
    "down_limit,up_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20)\n",
    "\n",
    "models=[LogisticRegression,\n",
    "       KNeighborsClassifier,\n",
    "       SVC,\n",
    "       MLPClassifier,\n",
    "       DecisionTreeClassifier,\n",
    "       RandomForestClassifier,\n",
    "       GradientBoostingClassifier,\n",
    "       LGBMClassifier,\n",
    "       XGBClassifier,\n",
    "       CatBoostClassifier]\n",
    "\n",
    "import time\n",
    "\n",
    "def fitter(x_train,x_test,y_train,y_test,model):\n",
    "        startt=time.time()\n",
    "        if model==CatBoostClassifier:\n",
    "            modelf=model().fit(x_train,y_train,verbose=False)\n",
    "        else:\n",
    "            modelf=model().fit(x_train,y_train)\n",
    "        y_pred=modelf.predict(x_test)\n",
    "        timer=time.time()-startt\n",
    "        ascore=cross_val_score(model(),x_train,y_train ,cv=10, scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "        f1score=cross_val_score(model(),x_train,y_train ,cv=10, scoring=\"f1\", n_jobs=-1).mean()\n",
    "        print(\"\"\"\n",
    "            Model:{}\n",
    "            accuracy_score:{}\n",
    "            f1_score:{}\n",
    "            fit&predict time:{}\n",
    "            \"\"\".format(model.__name__,ascore,f1score,timer))\n",
    "        print(\"-\"*60)\n",
    "\n",
    "\n",
    "for i in models:\n",
    "    fitter(x_train,x_test,y_train,y_test,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBM default params:\n",
    "#learning_rate=0.1\n",
    "#n_estimators=100\n",
    "#min_samples_split=2\n",
    "#min_samples_leaf=1\n",
    "#max_depth=3\n",
    "\n",
    "\n",
    "gbm_params={\n",
    "    \"learning_rate\":[0.01,0.1,0.3],\n",
    "    \"n_estimators\":[100,500,2000],\n",
    "    \"min_samples_split\":[2,4,6,7],\n",
    "    \"min_samples_leaf\":[1,2,3],\n",
    "    \"max_depth\":[3,5,7]\n",
    "}\n",
    "\n",
    "gbm_cv_model=GridSearchCV(GradientBoostingClassifier(),gbm_params,cv=5,verbose=2,n_jobs=-1).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params={\n",
    "    \"learning_rate\":[0.01,0.1,0.3],\n",
    "    \"num_leaves\":[25,31,40,50],\n",
    "    \"n_estimators\":[50,100,500,2000],\n",
    "    \"min_child_samples\":[10,20,30],\n",
    "    \"num_iterations\":[100,500,1000],\n",
    "    \"max_bin\":[255,305,400]\n",
    "}\n",
    "\n",
    "lgbm_cv_model=GridSearchCV(LGBMClassifier(),lgbm_params,cv=5,verbose=2,n_jobs=-1).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGBM early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tuned=LGBMClassifier(learning_rate=0.01,\n",
    "                         max_bin=255,\n",
    "                         min_child_samples=10,\n",
    "                         n_estimators=1000,\n",
    "                         num_iterations=500,\n",
    "                         num_leaves=31,\n",
    "                         ).fit(x_train,y_train,\n",
    "                                 eval_set=[(x_test,y_test)],\n",
    "                                 eval_metric=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance=pd.DataFrame(rf_tuned.feature_importances_*100,index=x_train.columns,columns=[\"importance\"])\n",
    "importance.sort_values(by=\"importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\n",
    "plt.xlabel(\"Variable Importance\")\n",
    "plt.gca().legend_=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "xtr_backup=x_train\n",
    "xte_backup=x_test\n",
    "\n",
    "\n",
    "sed=pp.scale(xtr_backup)\n",
    "sed_xtr=pd.DataFrame(sed,columns=x_train.columns,index=x_train.index)\n",
    "sed=pp.scale(xte_backup)\n",
    "sed_xte=pd.DataFrame(sed,columns=x_test.columns,index=x_test.index)\n",
    "\n",
    "\n",
    "x_train=sed_xtr\n",
    "x_test=sed_xte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svc_tuned.predict(x_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(true_y, pred_y, title='Confusion Matrix', figsize=(8,6)):\n",
    "    \"\"\" Custom function for plotting a confusion matrix for predicted results \"\"\"\n",
    "    conf_matrix = confusion_matrix(true_y, pred_y)\n",
    "    conf_df = pd.DataFrame(conf_matrix, columns=np.unique(true_y), index = np.unique(true_y))\n",
    "    conf_df.index.name = 'Actual'\n",
    "    conf_df.columns.name = 'Predicted'\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.title(title)\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(conf_df, cmap=\"Blues\", annot=True, \n",
    "                annot_kws={\"size\": 16}, fmt='g')\n",
    "    plt.show()\n",
    "    return\n",
    "plot_confusion_matrix(y_test,y_pred)\n",
    "\n",
    "ascore=accuracy_score(y_test,y_pred)\n",
    "f1score=f1_score(y_test,y_pred)\n",
    "\n",
    "print(\"SCORES\")\n",
    "print(\"\"\"accuracy_score:{}\n",
    "f1_score:{}\n",
    "\"\"\".format(ascore,f1score))\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
